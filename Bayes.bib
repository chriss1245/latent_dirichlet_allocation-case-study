
@misc{tomar_topic_2019,
	title = {Topic modeling using {Latent} {Dirichlet} {Allocation}({LDA}) and {Gibbs} {Sampling} explained!},
	url = {https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045},
	abstract = {How Topic Modelling works and how to implement it using LDA and Gibbs Sampling},
	language = {en},
	urldate = {2022-04-20},
	journal = {Analytics Vidhya},
	author = {Tomar, Ankur},
	month = jul,
	year = {2019},
}

@misc{noauthor_gibbs_nodate,
	title = {Gibbs {Sampler} for {LDA} - {Agustinus} {Kristiadi}},
	url = {https://agustinus.kristia.de/techblog/2017/09/07/lda-gibbs/},
	abstract = {Implementation of Gibbs Sampler for the inference of Latent Dirichlet Allocation (LDA)},
	language = {en},
	urldate = {2022-04-26},
	file = {Snapshot:files/10/lda-gibbs.html:text/html},
}

@misc{bergmann_inferring_2016,
	title = {Inferring the posteriors in {LDA} through {Gibbs} sampling},
	url = {https://tillbe.github.io/./lda-gibbs-toy.html},
	abstract = {In my last blog post, which was about a million years ago, I described the generative nature of LDA and left the interferential step open. In this blog post, I will explain one method to calculate estimations of the topic distribution θ and the term distribution ϕ. This approach, first formulated by Griffiths and Steyvers (2004) in the context of LDA, is to use Gibbs sampling, a common algorithm within the Markov Chain Monte Carlo (MCMC) family of sampling algorithms. Before applying Gibbs sampling directly to LDA, I will first give a short introduction to Gibbs sampling more generally.},
	language = {en},
	urldate = {2022-04-27},
	journal = {Till Bergmann},
	author = {Bergmann, Till},
	month = may,
	year = {2016},
	note = {Section: misc},
}

@misc{ch-13_computer_science_and_engineering_gibbs_2019,
	title = {Gibbs {Sampling} for {LDA}, {Applications}},
	url = {https://www.youtube.com/watch?v=i-X9KPQT0Jc},
	abstract = {Subject: Computer Science 
Course: Natural Language Processing},
	urldate = {2022-04-30},
	author = {{Ch-13 Computer Science and Engineering}},
	month = jul,
	year = {2019},
}

@misc{noauthor_emotion_nodate,
	title = {Emotion {Detection} from {Text}},
	url = {https://www.kaggle.com/dataset/042977506d4b87fe2ce6998514bd60df9ae2bdde98acf973acfd87e758e50d68},
	abstract = {Predict emotion from textual data : Multi-class text classification},
	language = {en},
	urldate = {2022-04-30},
	file = {Snapshot:files/14/emotion-detection-from-text.html:text/html},
}

@misc{chris_parameter_2021,
	title = {Parameter {Estimation} for {Latent} {Dirichlet} {Allocation} explained with {Collapsed} {Gibbs} {Sampling} in…},
	url = {https://medium.com/@datastories/parameter-estimation-for-latent-dirichlet-allocation-explained-with-collapsed-gibbs-sampling-in-1d2ec78b64c},
	abstract = {Latent Dirichlet Allocation (LDA), first published in Blei et al. (2003) is one of the most popular topic modeling approaches today. LDA…},
	language = {en},
	urldate = {2022-04-30},
	journal = {Medium},
	author = {Chris},
	month = mar,
	year = {2021},
	file = {Snapshot:files/16/parameter-estimation-for-latent-dirichlet-allocation-explained-with-collapsed-gibbs-sampling-in.html:text/html},
}
